---
title: 'MECH481A6: Engineering Data Analysis in R'
subtitle: 'Chapter 11 Homework: Modeling' 
author: 'Michael Thill'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: pdf_document
---

```{r global-options, include=FALSE}
# set global options for figures, code, warnings, and messages
knitr::opts_chunk$set(fig.width=6, fig.height=4, fig.path="../figs/",
                      echo=FALSE, warning=FALSE, message=FALSE)
```

# Load packages

```{r load-packages, message=FALSE}
# load packages for current session
library(tidyverse) 
library(gridExtra) 
library(car)
```

# Chapter 11 Homework

This homework will give you experience with OLS linear models and testing their assumptions.  

For this first problem set, we will examine issues of ***collinearity among predictor variables*** when fitting an OLS model with two variables. As you recall, assumption 3 from OLS regression requires there be no *collinearity* among predictor variables (the $X_i$'s) in a linear model.  The reason is that the model struggles to assign the correct $\beta_i$ values to each predictor when they are strongly correlated.   

## Question 1
Fit a series of three linear models on the `bodysize.csv` data frame using `lm()` with `height` as the dependent variable:  
  1. Model 1: use `waist` as the independent predictor variable:  
        - `formula = height ~ waist`   
  2. Model 2: use `mass` as the independent predictor variable:  
        - `formula = height ~ mass`  
  3. Model 3: use `mass + waist` as a linear combination of predictor variables:  
        - `formula = waist + mass`  
    
Report the coefficients for each of these models.  What happens to the sign and magnitude of the `mass` and `waist` coefficients when the two variables are included together?  Contrast that with the coefficients when they are used alone.

Evaluate assumption 3 about whether there is collinearity among these variables.  Do you trust the coefficients from model 3 after having seen the individual coefficients reported in models 1 and 2?


```{r ch11-homework-q1, echo=FALSE, include=FALSE}
# Load the bodysize data
bodysize <- read_csv("../data/bodysize.csv")

# Model 1: height ~ waist
model_1 <- lm(height ~ waist, data = bodysize)

# Model 2: height ~ mass
model_2 <- lm(height ~ mass, data = bodysize)

# Model 3: height ~ waist + mass
model_3 <- lm(height ~ waist + mass, data = bodysize)

# Show coefficients cleanly
coef_1 <- coef(model_1)
coef_2 <- coef(model_2)
coef_3 <- coef(model_3)

coef_1
coef_2
coef_3
```

*** Model_1 intercept 155.4967199 coefficient 0.1099283, Model_2 intercept 150.6109083 coefficient 0.1909431, Model_3 intercept 177.4604248 coefficient -.6347416, .6394358***

## Question 2
Create a new variable in the `bodysize` data frame using `dplyr::mutate`. Call this variable `volume` and make it equal to $waist^2*height$.  Use this new variable to predict `mass`.  

```{r ch11-homework-q2}

bodysize <- bodysize %>%
  mutate(volume = waist^2 * height)

mod_vol <- lm(mass ~ volume, data = bodysize)

summary(mod_vol)

```

Does this variable explain more of the variance in `mass` from the NHANES data? How do you know? (hint: there is both *process* and *quantitative* proof here)

```{r ch11-homework-q2a}

summary(lm(mass ~ waist, data = bodysize))
summary(lm(mass ~ height, data = bodysize))
summary(mod_vol)  

```
*** This variable does explain the variance better then the previous models. This can be seen by the R squared value being higher in this version when compared to the other models generated and the respective R squared values they have *** 

Create a scatter plot of `mass` vs. `volume` to examine the fit.  Draw a fit line using `geom_smooth()`.

```{r ch11-homework-q2b}
ggplot(bodysize, aes(x = volume, y = mass)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", linewidth = 1) +
  theme_bw() +
  labs(
    title = "Mass vs. Volume",
    x = "Volume (waist² × height)",
    y = "Mass"
  )

```

## Question 3
Load the `cal_aod.csv` data file and fit a linear model with `aeronet` as the independent variable and `AMOD` as the independent variable. 
```{r ch11-homework-q3}
# load data
cal <- read_csv("../data/cal_aod.csv")

model_cal <- lm(amod ~ aeronet, data = cal)

summary(model_cal)

```

Evaluate model assumptions 4-7 from the coursebook.  Are all these assumptions valid? 

```{r ch11-homework-q3a}
#assumption 4: mean of residuals is zero
residual <- residuals(model_cal)
fit <- fitted(model_cal)

mean_residual <- mean(residual)
mean_residual

```
*** our assumptions are valid because the residual is basically zero for this model.***
```{r ch11-homework-q3b}
#assumption 5: residuals are normally distributed
qqnorm(residual)
qqline(residual, col = "red")

```
*** The fitted line fits the distribution plot very closely with the exception when you are looking at either end of the data where the data drops bellow on the left and and is above on the right end. ***

```{r ch11-homework-q3c}
#assumption 6: the error term is homoscedastic
plot(fit, residual,
     xlab = "Fitted values",
     ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red")

```
*** This assumption is valid for most locations as the data shows the residuals are all close to zero.
```{r ch11-homework-q3d}
#assumption 7: no autocorrelation among residuals
acf(residual, main = "ACF of Residuals")
pacf(residual,main = "Partial ACF od Residuals")

car::durbinWatsonTest(model_cal)

```
*** The Durbin-Watson test is very close to zero and all values fall within the bounds except for the first one for the ACF. ***